---
title: "BANA6043-Final Project"
output:
  html_document:
    df_print: paged
---

## Group Details:
+ Shama Dixit
+ Shreeya Idagunji
+ Haritha Sajimon
+ Vibhash Dhar

---  

# {.tabset .tabset-fade .tabset-pills}

## 1. Exploratory Data Analysis

### DATA DESCRIPTION

Here,we have used a UK Used Car Dataset spanning from 1997 to 2020. The dataset was obtained from Kaggle(https://www.kaggle.com/adityadesai13/used-car-dataset-ford-and-mercedes ). The dataset basically contains information about different models of used audi cars in UK and provides insights about the resale value of cars. The variables in this dataset are :

+  Model : This is a categorical variable including the names of all models of audi cars evaluated in the dataset.
+ Year : This is a numeric variable spanning from 1997 to 2020 and includes the year of initial purchase of each car.
+ Transmission :This is a categorical variable which states whether a particular car is Manual/Automatic/Semi-Auto.
+ Price : This is a numerical variable which indicates the resale price of each car.
+ Mileage : This is a numerical variable which indicates number of miles traveled by each car.
+ FuelType : This is a categorical variable which has three levels, Diesel/Hybrid/Petrol.
+ Tax: This is a numeric variable which indicates the onroad tax to be paid.
+ mpg :This is a numeric variable which is the distance measured in miles that the car can travel per gallon of fuel.
+ EngineSize : This is a numeric variable which is the measurement of total volume of cylinders in the engine. Bigger the engine size,more space there is for air and fuel inside it.

OBJECTIVE : We are trying to observe the influence of predictors on the resale price of cars.

Below is a summary of the dataset.

---

#### New Functions/Packages Used

We have used the following new functions & packages:

+ Function **preProcess** to normalize the Data.
+ **glmnet** package for Ridge, Lasso and Elastic Net Regression
+ **caret** package for introducing custom parameters in models.

---


#### Load Required Packages and read the csv    

```{r message=FALSE, error=FALSE, warning=FALSE}
library(tidyverse)
library(GGally)
library(randomForest)
library(e1071)
library(caret)
library(glmnet)
library(mlbench)
library(psych)

d <- read.csv("audi.csv")
summary(d)
str(d)
```

---

#### DATA CLEANING

In order to check for null values in our dataset, we have included the following. We could see that there are no null values in our dataset.

```{r}
sum(is.na(d$model))
sum(is.na(d$year))
sum(is.na(d$price))
sum(is.na(d$transmission))
sum(is.na(d$mileage))
sum(is.na(d$fuelType))
sum(is.na(d$tax))
sum(is.na(d$mpg))
sum(is.na(d$engineSize))

```


#### VISUALIZATIONS AND DATA ANALYSIS


```{r message=FALSE, error=FALSE, warning=FALSE}
library(sqldf)
sql1 <- sqldf('select year,avg(price) from d group by year')
sql1

sql2 <- sqldf('select model,avg(price) from d group by model order by avg(price)')
sql2

sql3 <- sqldf('select transmission,avg(price) from d group by transmission')
sql3

sql4 <- sqldf('select fuelType,avg(price) from d group by fuelType')
sql4

ggplot(data=d)+geom_bar(mapping=aes(x=year),fill= "blue")+ggtitle("No: of cars purchased in each year")


ggplot(data=d)+geom_bar(mapping=aes(x=model),fill= "red")+ggtitle("No: of cars of each model")

ggplot(data=d)+geom_bar(mapping=aes(x=transmission),fill= "brown")+ggtitle("No: of cars of each transmission type")

ggplot(data=d)+geom_bar(mapping=aes(x=fuelType),fill= "green")+ggtitle("No: of cars of different fueltypes")

```

---

#### **Observations:**
+ Here, we found average price of cars in each year and could observe that the average price was minimum in the year 2002 and maximum in the year 2020. 
+ The average price of cars was the lowest for the A2 model and highest for the R8 model.
+ Automatic type cars had the highest average resale price ,whereas manual type cars had the lowest average resale price.
+ Petrol cars had the lowest average resale price ,whereas hybrid model cars had the highest average resale price.  

---

#### CORRELATION MATRIX AND SCATTER PLOTS

```{r}
library(tidyverse)
library(ggplot2)
library(GGally)
cor(d %>% select(year,price,mileage,tax,mpg,engineSize))
hist(d$price)
f <- d %>% mutate(lnprice =log(price))
hist(f$lnprice)

ggpairs(d %>% select(year,price))

ggpairs(d %>% select(price,mileage))

ggpairs(d %>% select(price,tax))

ggpairs(d %>% select(price,mpg))

ggpairs(d %>% select(price,engineSize))

```

#### **Observations:**
+ The year and price variables appear to be positively correlated as shown by scatter plot and correlation coefficients.
+ The price and mileage variables appear to be negatively correlated as shown by scatter plot and correlation coefficients.
+ The price and tax variables appear to be positively correlated as shown by scatter plot and correlation coefficients.
+ The price and mpg variables appear to be negatively correlated as shown by scatter plot and correlation coefficients.
+ The price and engineSize variables appear to be positively correlated as shown by scatter plot and correlation coefficients.

---  

#### CORRELATION MATRIX

```{r message=FALSE, warning=FALSE}
data <- read.csv('audi.csv', header=TRUE)
ggpairs(data %>% select(price, year, transmission, mileage, fuelType, tax, mpg, engineSize), bins=10)
```

---



## 2. Splitting and Manipulating the data  

```{r}
set.seed(123)

idx <- sample(nrow(data), .7*nrow(data))
train <- data[idx, ]
test <- data[-idx, ]
str(train)
str(test)
```


---  

#### Manipulate the data and scale it using preProcess function in caret package.  

```{r}
col_n <- c('mpg', 'mileage', 'tax', 'engineSize')

proc <- preProcess(train[,col_n], method = c('center', 'scale'))

train[,col_n] <- predict(proc, train[,col_n])
test[,col_n] <- predict(proc, test[,col_n])

summary(train)

```


---

## 3. Simple Linear Model

The simplest form of regression is linear regression, which assumes that the predictors have a linear relationship with the target variable. The input variables are assumed to have a Gaussian distribution and are not correlated with each other (a problem called multi-collinearity).  

The linear regression equation can be expressed in the following form: y = a1x1 + a2x2 + a3x3 + ..... + anxn + b  

In the above equation:

y is the target variable.
x1, x2, x3, ... xn are the features.  
a1, a2, a3, ... an are the coefficients.  
b is the parameter of the model.    

The parameters a and b in the model are selected through the ordinary least squares (OLS) method. This method works by minimizing the sum of squares of residuals (actual value - predicted value).

```{r}
linear_model <- lm(price ~ ., data = train)
summary(linear_model)
hist(linear_model$residuals, main = 'Distribution of Residuals', col = 'pink')
```


---

#### Use the model and test data to predict the prices  


```{r}
preds_lm <- predict(linear_model, newdata = test)
```


---  


#### Performance of the model on the basis of RMSE, MAE and RSquared  


```{r}
calc_performance <- function(actual, predicted) {
  rmse <- sqrt(mean((actual - predicted)**2))
  mae <- mean(abs(actual - predicted))
  rsq <- 1 - (sum((actual - predicted)**2))/(sum((actual - mean(actual))**2))
  return(c(RMSE = rmse, MAE = mae, RSquared = rsq))
}
calc_performance(train$price, linear_model$fitted.values)
calc_performance(test$price, preds_lm)
```

  
RMSE and MAE values for train and test datasets are almost similar, thus we can say that model performance is reasonably good.  

Now, let us evaluate other regression models to see if we can come up with a better model for price prediction.  

---


## 4. Define Custom Control Parameters  

We will use trainControl function present in caret package to define custom control parameters for regression models.

We have used the below parameters for creating a custom control:

1. method = 'repeatedcv'; CV: Cross Validation
2. number = 10; In 10 fold cross validation, training data is split into 10 parts out of which 9 are used for model building and 1 is used for error estimation. 
3. repeat = 10; CV is repeated 10 times.

```{r}
custom <- trainControl(method = 'repeatedcv',
                       number = 10,
                       repeats = 10)
```

## 5. Linear, Ridge, Lasso and Elastic Net Regression Models {.tabset .tabset-fade .tabset-pills}

### 5.1 Linear Regression Model using custom control parameters  


```{r warning=FALSE, message=FALSE}
set.seed(123)
lm <- train(price~., train, method = 'lm', trControl = custom)

lm$results
lm
summary(lm)
plot(lm$finalModel, col = 'blue')
```

From the summary, it is clear that all the predictors are statistically significant

##### **In the model, the training data is split into 10 parts and 9 parts are used to train the model and 1 part is used to estimate the error. The RMSE values shown here are taken for 1 part of testing data. Comapring it to Simple Linear Model, the RMSE for test data has improved after using custom control.**

---


### 5.2 Ridge Regression Model

Ridge regression is an extension of linear regression where the loss function is modified to minimize the complexity of the model. This modification is done by adding a penalty parameter that is equivalent to the square of the magnitude of the coefficients.  

**Loss function = OLS + alpha * summation (squared coefficient values)**  


Ridge Regression tries to shrink the coefficients by trying to shrink the coefficients, but keeps all the variables in the model.  

Ridge regression is also referred to as l2 regularization. The arguments used in the model are:  


**alpha**: determines the weighting to be used. In case of ridge regression, the value of alpha is zero.

**family**: determines the distribution family to be used. Since this is a regression model, we will use the Gaussian distribution.

**lambda**: determines the lambda values to be tried. lambda is the strength of the penalty on the coefficients.  


```{r}
set.seed(123)
ridge <- train(price ~ ., train, method = 'glmnet',
               tuneGrid = expand.grid(alpha=0,
                                      lambda= seq(100, 1000, length=10)),
                                      trControl = custom)
ridge
plot(ridge, col = 'red', main='Plot of RMSE vs Lambda')
plot(ridge$finalModel, xvar='lambda', label=TRUE)
```


##### **Observations:**

+ We can see that RMSE is constant till lambda value of 700, after which is starts to increase. So, the model has selected lambda value of 700.  
+ We can see from plot of coefficients vs Log Lambda that as Lambda tends to 15, all coefficients become more or less equal to 0.
+ The plot shows that at all times all 34 variables are present in the model(see top part of Coefficients vs Log Lambda plot).

---  

### 5.3 LASSO Regression  

Lasso regression, or the Least Absolute Shrinkage and Selection Operator, is also a modification of linear regression. In lasso, the loss function is modified to minimize the complexity of the model by limiting the sum of the absolute values of the model coefficients.  

Alpha value for LASSO is 1.


```{r}
set.seed(123)
lasso <- train(price ~ ., train, 
               method = 'glmnet',
               tuneGrid = expand.grid(alpha=1,
                                      lambda= seq(1, 5, length=20)),
                                      trControl = custom)

lasso
plot(lasso, col = 'red', main='Plot of RMSE vs Lambda')
plot(lasso$finalModel, xvar='lambda', label=TRUE)
```

##### **Observations:**

+ We can see that RMSE is constant till lambda value of approximately 4.6, after which is starts to increase. So, the model has selected lambda value of 4.6.  
+ We can see from plot of coefficients vs Log Lambda that as Log Lambda tends to ~9, all coefficients become more or less equal to 0.
+ The plot shows that LASSO deflates the number of variables and as Log Lambda tends to > 8 only 4 Variables are left in the model.

---  

### 5.4. Elastic Net Regression

Elastic net regression combines the properties of ridge and lasso regression. It works by penalizing the model using both the 1l2-norm1 and the 1l1-norm1. The model can be easily built using the caret package.

```{r}
set.seed(123)
en <- train(price ~ ., train, 
               method = 'glmnet',
               tuneGrid = expand.grid(alpha=seq(0, 1, length=10),
                                      lambda= seq(0, 30, length=15)),
                                      trControl = custom)
en
plot(en, main='Plot of RMSE vs Lambda')
plot(en$finalModel, xvar='lambda', label=TRUE)
```


##### **Observations:**

+ The model chooses alpha value of 0.1111 and lambda value of 25.7143.
+ Plot of RMSE vs Lambda shows mixing percentages and we can see the chosen combination of alpha = 0.1111 and lambda = 25.7143 denoted by the lowest line in the plot.
+ In the plot of Log Lambda vs Coefficients, we can see that when Log Lambda tends to 10, almost all coefficients become 0 and we are left with only 8 variables in the model.


---

## 6. Comparing the Models  

```{r}
model_list <- list(LinearModel = lm, Ridge = ridge, Lasso = lasso, ElasticNet = en)
res <- resamples(model_list)
summary(res)
```

##### **Linear Model performs better than the Ridge, LASSO and Elastic Net for this dataset. Linear Model has high mean RSquared value and lowest mean RMSE**

---

### Predict Prices using the models

```{r}
pred_MLinear <- predict(lm, newdata = test)
pred_Ridge <- predict(ridge, newdata = test)
pred_Lasso <- predict(lasso, newdata = test)
pred_EN <- predict(en, newdata = test)

#include SImple Linear Model also for comparison
calc_performance(test$price, preds_lm)
calc_performance(test$price, pred_MLinear)
calc_performance(test$price, pred_Ridge)
calc_performance(test$price, pred_Lasso)
calc_performance(test$price, pred_EN)
```

##### **Observations:**
+ Both the linear models have similar values for RMSE, MAE and RSquared, 
+ Elastic net model comes very close to Linear Model's accuracy and has similar RSquared value, lower MAE and slightly higher RMSE.
+ Ridge and Lasso individually don't stand any chance against linear models and elastic net Model.
+ We would choose linear model as the best model here because of its ease of implementation and simplicity. 


---

## 7. Conclusion

In the project, we have explored linear regression models like Ridge, LASSO and Elastic Net. We also learned about the regularization techniques to get around the Multi-collinearity issue  We explored custom parameters such as Cross Validation. 

The performance of the Models is summarized below:

+ **Simple Linear Model**:  RMSE was 3911.10 and RSquared of 0.8911
+ **Linear Regression using CUstom Control**: RMSE was 3911.10 and RSquared of 0.8911
+ **Ridge Regression Model**: RMSE was 3971.82 and RSquared of 0.8877
+ **LASSO Regression Model**: RMSE was 3913.66 and RSquared of 0.8910
+ **Elastic Net Regression Model**: RMSE was 3912.97 and RSquared of 0.8910

For the Data that we have, Linear Models are performing much better than the regularized models. Only Elastic Net Regression came really close to Linear Models in terms of performance.
Linear Models provide RMSE of 3911 and RSquared value of 0.8911, so we would choose Linear Models for predicting used car prices as a function of model, mpg, mileage, engineSize, tax, transmission and year.

































